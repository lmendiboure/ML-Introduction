# TP5 â€“ RÃ©gularisation et gÃ©nÃ©ralisation dans les CNN (CIFARâ€‘10)

## Objectifs
- Appliquer un CNN Ã  un nouveau jeu de donnÃ©es plus rÃ©aliste : CIFARâ€‘10.
- Comprendre et comparer plusieurs mÃ©thodes de rÃ©gularisation : Dropout, L2, BatchNorm, Data Augmentation, EarlyStopping.
- Observer leurs effets sur les performances et les courbes dâ€™apprentissage.

Environnement : Googleâ€¯Colab (TensorFlow / Keras) 

**Remarque : Sans GPU activitÃ©, les temps seront trÃ¨s longs !**

---

## Ã‰tapeâ€¯0 â€“ Chargement et exploration du dataset

```python
import tensorflow as tf
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.regularizers import l2
import numpy as np
import matplotlib.pyplot as plt

print("TensorFlow:", tf.__version__)
print("GPU:", tf.config.list_physical_devices('GPU'))

# Chargement
(X_train, y_train), (X_test, y_test) = cifar10.load_data()

# Normalisation
X_train = X_train.astype("float32")/255.0
X_test  = X_test.astype("float32")/255.0

# One-hot
y_train_cat = to_categorical(y_train, 10)
y_test_cat  = to_categorical(y_test, 10)
```

**Q0.1.** Quelle est la taille et la structure des imagesâ€¯?  
**Q0.2.** Combien de classes contient CIFARâ€‘10â€¯?  

ğŸ“š *Aideâ€¯: [Keras CIFARâ€‘10 Dataset](https://keras.io/api/datasets/cifar10/)*  

### Visualisation dâ€™exemples
```python
labels = ["airplane","automobile","bird","cat","deer","dog","frog","horse","ship","truck"]
for i in range(6):
    plt.subplot(1,6,i+1)
    plt.imshow(X_train[i])
    plt.title(labels[int(y_train[i])])
    plt.axis("off")
plt.show()
```
**Q0.3.** Quelle diffÃ©rence majeure avec MNIST remarquesâ€‘tu (taille, couleur, complexitÃ©)â€¯?  

---

## Ã‰tapeâ€¯1 â€“ CNN de base (sans rÃ©gularisation)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

base = Sequential([
    Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),
    MaxPooling2D((2,2)),
    Conv2D(64,(3,3),activation='relu'),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(128,activation='relu'),
    Dense(10,activation='softmax')
])

base.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history_base = base.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20, batch_size=128, verbose=1
)
```

**Q1.** Quelle prÃ©cision obtiensâ€‘tu sur le train et sur le testâ€¯?  
**Q2.** Observe les courbes dâ€™accuracy : y aâ€‘tâ€‘il un overfittingâ€¯? Si oui, Ã  quel moment se produit-il ?  

```python
plt.plot(history_base.history['accuracy'],label='train')
plt.plot(history_base.history['val_accuracy'],label='val')
plt.legend(); plt.title("CNN sans rÃ©gularisation")
plt.show()
```

---

## Ã‰tapeâ€¯2 â€“ Dropout

```python
from tensorflow.keras.layers import Dropout

drop = Sequential([
    Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),
    MaxPooling2D(2,2),
    Dropout(0.25),
    Conv2D(64,(3,3),activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128,activation='relu'),
    Dropout(0.5),
    Dense(10,activation='softmax')
])

drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_drop = drop.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20, batch_size=128, verbose=1
)
```

**Q3.** Qe signiie le Dropout ? Compare les courbes train/val : lâ€™Ã©cart diminueâ€‘tâ€‘ilâ€¯?  
**Q4.** Que se passeâ€‘tâ€‘il si le taux de Dropout est trop Ã©levÃ©â€¯?  

ğŸ“š *Aideâ€¯: [Dropout â€“ Keras](https://keras.io/api/layers/regularization_layers/dropout/)*

---

## Ã‰tapeâ€¯3 â€“ RÃ©gularisationâ€¯L2

```python
from tensorflow.keras.regularizers import l2

l2_model = Sequential([
    Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),
    MaxPooling2D(2,2),
    Conv2D(64,(3,3),activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128,activation='relu',kernel_regularizer=l2(0.001)),
    Dense(10,activation='softmax')
])

l2_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_l2 = l2_model.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20, batch_size=128, verbose=1
)
```

**Q5.** Qu'est ce que la rÃ©gularisation L2 ? Quel impact : Lâ€™Ã©cart train/test diminueâ€‘tâ€‘ilâ€¯?  
**Q6.** Quel effet aurait une rÃ©gularisation trop forteâ€¯?  

ğŸ“š *Aideâ€¯: [Regularizers â€“ Keras](https://keras.io/api/layers/regularizers/)*

---

## Ã‰tapeâ€¯4 â€“ Batchâ€¯Normalization

```python
from tensorflow.keras.layers import BatchNormalization

bn = Sequential([
    Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Conv2D(64,(3,3),activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128,activation='relu'),
    Dense(10,activation='softmax')
])

bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_bn = bn.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20, batch_size=128, verbose=1
)
```

**Q7.** Qu'est ce que la Batch Normalization ? La convergence estâ€‘elle plus rapide ou plus stableâ€¯?  
**Q8.** Pourquoi cette normalisation aideâ€‘tâ€‘elleâ€¯?  

ğŸ“š *Aideâ€¯: [BatchNormalization â€“ Keras](https://keras.io/api/layers/normalization_layers/batch_normalization/)*

---

## Ã‰tapeâ€¯5 â€“ Dataâ€¯Augmentation

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True
)

train_gen = datagen.flow(X_train, y_train_cat, batch_size=128)

aug = tf.keras.models.clone_model(base)
aug.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history_aug = aug.fit(
    train_gen,
    validation_data=(X_test, y_test_cat),
    epochs=20, verbose=1
)
```

**Q9.** Qu'est ce que la Data Augmentation ? Pourquoi cette mÃ©thode rÃ©duitâ€‘elle lâ€™overfittingâ€¯?  
**Q10.** Quelles transformations semblent les plus efficacesâ€¯?  **Si c'est trop long, rÃ©duisez Ã©ventuellement le nombre d'Epochs**

ğŸ“š *Aideâ€¯: [ImageDataGenerator â€“ Keras](https://keras.io/api/preprocessing/image/)*

---

## Ã‰tapeâ€¯6 â€“ Earlyâ€¯Stopping

```python
from tensorflow.keras.callbacks import EarlyStopping

early = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

es = tf.keras.models.clone_model(base)
es.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_es = es.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=50, batch_size=128, callbacks=[early], verbose=1
)
```

**Q11.** Combien dâ€™Ã©poques sont rÃ©ellement effectuÃ©esâ€¯?  
**Q12.** Qu'est ce que l'Early Stopping ? Quel intÃ©rÃªt par rapport Ã  un nombre fixe dâ€™Ã©poquesâ€¯?  

ğŸ“š *Aideâ€¯: [EarlyStopping â€“ Keras](https://keras.io/api/callbacks/early_stopping/)*

---

## Ã‰tapeâ€¯7 â€“ Comparaison et analyse

| MÃ©thode | Accuracyâ€¯(train) | Accuracyâ€¯(test) | Ã‰cart rÃ©duitâ€¯? | Commentaire |
|----------|------------------|------------------|----------------|--------------|
| Aucune | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| Dropout | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| L2 | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| BatchNorm | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| Dataâ€¯Aug | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| Earlyâ€¯Stop | â€¦ | â€¦ | â˜ Oui â˜ Non |  |

**Q13.** Quelle mÃ©thode te semble la plus efficaceâ€¯?  
**Q14.** Peutâ€‘on les combinerâ€¯? Si oui, commentâ€¯?  

---

## SynthÃ¨se

Ce TP tâ€™a permis deâ€¯:
- ExpÃ©rimenter les principales techniques de rÃ©gularisation sur un dataset rÃ©aliste (CIFARâ€‘10).  
- Observer leurs effets concrets sur les performances et la stabilitÃ©.  
- Comprendre que gÃ©nÃ©raliser, câ€™est trouver lâ€™Ã©quilibre entre capacitÃ© et contrÃ´le.
