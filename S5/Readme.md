# TP5 â€“ RÃ©gularisation et gÃ©nÃ©ralisation dans les CNN (CIFARâ€‘10)

## Objectifs
- Appliquer un CNN Ã  un nouveau jeu de donnÃ©es plus rÃ©aliste : CIFARâ€‘10.
- Comprendre et comparer plusieurs mÃ©thodes de rÃ©gularisation : Dropout, L2, BatchNorm, Data Augmentation, EarlyStopping.
- Observer leurs effets sur les performances et les courbes dâ€™apprentissage.

Environnement : Googleâ€¯Colab (TensorFlow / Keras)

---

## Ã‰tapeâ€¯0 â€“ Chargement et exploration du dataset

```python
from tensorflow.keras.datasets import cifar10
from tensorflow.keras.utils import to_categorical
import matplotlib.pyplot as plt

(X_train, y_train), (X_test, y_test) = cifar10.load_data()

print("Taille :", X_train.shape, X_test.shape)

# TODO : normaliser les pixels entre 0 et 1
# TODO : encoder les labels (oneâ€‘hot)
```

**Q0.1.** Quelle est la taille et la structure des imagesâ€¯?  
**Q0.2.** Combien de classes contient CIFARâ€‘10â€¯?  

ğŸ“š *Aideâ€¯: [Keras CIFARâ€‘10 Dataset](https://keras.io/api/datasets/cifar10/)*  

### Visualisation dâ€™exemples
```python
labels = ["airplane","automobile","bird","cat","deer","dog","frog","horse","ship","truck"]
for i in range(6):
    plt.subplot(1,6,i+1)
    plt.imshow(X_train[i])
    plt.title(labels[int(y_train[i])])
    plt.axis("off")
plt.show()
```
**Q0.3.** Quelle diffÃ©rence majeure avec MNIST remarquesâ€‘tu (taille, couleur, complexitÃ©)â€¯?  

---

## Ã‰tapeâ€¯1 â€“ CNN de base (sans rÃ©gularisation)

```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

base = Sequential([
    Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),
    MaxPooling2D((2,2)),
    Conv2D(64,(3,3),activation='relu'),
    MaxPooling2D((2,2)),
    Flatten(),
    Dense(128,activation='relu'),
    Dense(10,activation='softmax')
])

base.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history_base = base.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20, batch_size=128, verbose=1
)
```

**Q1.** Quelle prÃ©cision obtiensâ€‘tu sur le train et sur le testâ€¯?  
**Q2.** Observe les courbes dâ€™accuracy : y aâ€‘tâ€‘il un overfittingâ€¯?  

```python
plt.plot(history_base.history['accuracy'],label='train')
plt.plot(history_base.history['val_accuracy'],label='val')
plt.legend(); plt.title("CNN sans rÃ©gularisation")
plt.show()
```

---

## Ã‰tapeâ€¯2 â€“ Dropout

```python
from tensorflow.keras.layers import Dropout

drop = Sequential([
    Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),
    MaxPooling2D(2,2),
    Dropout(0.25),
    Conv2D(64,(3,3),activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128,activation='relu'),
    Dropout(0.5),
    Dense(10,activation='softmax')
])

drop.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_drop = drop.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20, batch_size=128, verbose=1
)
```

**Q3.** Compare les courbes train/val : lâ€™Ã©cart diminueâ€‘tâ€‘ilâ€¯?  
**Q4.** Que se passeâ€‘tâ€‘il si le taux de Dropout est trop Ã©levÃ©â€¯?  

ğŸ“š *Aideâ€¯: [Dropout â€“ Keras](https://keras.io/api/layers/regularization_layers/dropout/)*

---

## Ã‰tapeâ€¯3 â€“ RÃ©gularisationâ€¯L2

```python
from tensorflow.keras.regularizers import l2

l2_model = Sequential([
    Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),
    MaxPooling2D(2,2),
    Conv2D(64,(3,3),activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128,activation='relu',kernel_regularizer=l2(0.001)),
    Dense(10,activation='softmax')
])

l2_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_l2 = l2_model.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20, batch_size=128, verbose=1
)
```

**Q5.** Lâ€™Ã©cart train/test diminueâ€‘tâ€‘ilâ€¯?  
**Q6.** Quel effet aurait une rÃ©gularisation trop forteâ€¯?  

ğŸ“š *Aideâ€¯: [Regularizers â€“ Keras](https://keras.io/api/layers/regularizers/)*

---

## Ã‰tapeâ€¯4 â€“ Batchâ€¯Normalization

```python
from tensorflow.keras.layers import BatchNormalization

bn = Sequential([
    Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Conv2D(64,(3,3),activation='relu'),
    BatchNormalization(),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128,activation='relu'),
    Dense(10,activation='softmax')
])

bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_bn = bn.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=20, batch_size=128, verbose=1
)
```

**Q7.** La convergence estâ€‘elle plus rapide ou plus stableâ€¯?  
**Q8.** Pourquoi cette normalisation aideâ€‘tâ€‘elleâ€¯?  

ğŸ“š *Aideâ€¯: [BatchNormalization â€“ Keras](https://keras.io/api/layers/normalization_layers/batch_normalization/)*

---

## Ã‰tapeâ€¯5 â€“ Dataâ€¯Augmentation

```python
from tensorflow.keras.preprocessing.image import ImageDataGenerator

datagen = ImageDataGenerator(
    rotation_range=10,
    width_shift_range=0.1,
    height_shift_range=0.1,
    horizontal_flip=True
)

train_gen = datagen.flow(X_train, y_train_cat, batch_size=128)

aug = tf.keras.models.clone_model(base)
aug.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

history_aug = aug.fit(
    train_gen,
    validation_data=(X_test, y_test_cat),
    epochs=20, verbose=1
)
```

**Q9.** Pourquoi cette mÃ©thode rÃ©duitâ€‘elle lâ€™overfittingâ€¯?  
**Q10.** Quelles transformations semblent les plus efficacesâ€¯?  

ğŸ“š *Aideâ€¯: [ImageDataGenerator â€“ Keras](https://keras.io/api/preprocessing/image/)*

---

## Ã‰tapeâ€¯6 â€“ Earlyâ€¯Stopping et synthÃ¨se

```python
from tensorflow.keras.callbacks import EarlyStopping

early = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

es = tf.keras.models.clone_model(base)
es.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
history_es = es.fit(
    X_train, y_train_cat,
    validation_data=(X_test, y_test_cat),
    epochs=50, batch_size=128, callbacks=[early], verbose=1
)
```

**Q11.** Combien dâ€™Ã©poques sont rÃ©ellement effectuÃ©esâ€¯?  
**Q12.** Quel intÃ©rÃªt par rapport Ã  un nombre fixe dâ€™Ã©poquesâ€¯?  

ğŸ“š *Aideâ€¯: [EarlyStopping â€“ Keras](https://keras.io/api/callbacks/early_stopping/)*

---

## Ã‰tapeâ€¯7 â€“ Comparaison et analyse

| MÃ©thode | Accuracyâ€¯(train) | Accuracyâ€¯(test) | Ã‰cart rÃ©duitâ€¯? | Commentaire |
|----------|------------------|------------------|----------------|--------------|
| Aucune | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| Dropout | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| L2 | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| BatchNorm | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| Dataâ€¯Aug | â€¦ | â€¦ | â˜ Oui â˜ Non |  |
| Earlyâ€¯Stop | â€¦ | â€¦ | â˜ Oui â˜ Non |  |

**Q13.** Quelle mÃ©thode te semble la plus efficaceâ€¯?  
**Q14.** Peutâ€‘on les combinerâ€¯? Si oui, commentâ€¯?  

---

## SynthÃ¨se

Ce TP tâ€™a permis deâ€¯:
- ExpÃ©rimenter les principales techniques de rÃ©gularisation sur un dataset rÃ©aliste (CIFARâ€‘10).  
- Observer leurs effets concrets sur les performances et la stabilitÃ©.  
- Comprendre que gÃ©nÃ©raliser, câ€™est trouver lâ€™Ã©quilibre entre capacitÃ© et contrÃ´le.
