# TP â€“ SÃ©ance 1 (version active & guidÃ©e)  
## Premiers pas en apprentissage supervisÃ© avec scikit-learn

### ğŸ¯ Objectifs
- Explorer un **vrai dataset** (Breast Cancer) et comprendre sa structure (lignes, colonnes, classes).  
- DÃ©couvrir un algorithme simple : **k plus proches voisins (k-NN)**.  
- Ã‰valuer un modÃ¨le : **accuracy** et **matrice de confusion**.  
- Comparer avec la **rÃ©gression logistique**.  
- Tester et **justifier** des choix de paramÃ¨tres (valeurs de *k*, normalisation, etc.).  

â³ DurÃ©e cible : **~2h**  
ğŸ› ï¸ Plateforme : **Google Colab** (ou local avec Python 3 + scikit-learn).  

---

## ğŸ§­ RepÃ¨res utiles (oÃ¹ chercher lâ€™info ?)
- Guide de dÃ©marrage : https://scikit-learn.org/stable/getting_started.html  
- `KNeighborsClassifier` : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html  
- `LogisticRegression` : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html  
- `Pipeline` : https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html  
- `StandardScaler` : https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html  
- `train_test_split` : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html  
- `ConfusionMatrixDisplay` : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html  

Astuce Colab : vous pouvez afficher lâ€™aide courte dâ€™une classe avec `KNeighborsClassifier?` (ou `help(KNeighborsClassifier)`).

---

## Ã‰tape 0 â€” PrÃ©parer lâ€™environnement
CrÃ©ez un nouveau notebook et exÃ©cutez :

```python
import numpy as np, pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
```

**Q1.** Ã€ quoi servent `Pipeline` et `StandardScaler` dans un flux de travail ML ?  
ğŸ‘‰ Indice : lisez les courts rÃ©sumÃ©s des docs liÃ©es ci-dessus.

---

## Ã‰tape 1 â€” Explorer rapidement le dataset
```python
data = load_breast_cancer()
X, y = data.data, data.target
feature_names = data.feature_names
target_names = data.target_names

print("X shape:", X.shape)             # (n_samples, n_features)
print("y shape:", y.shape)
print("Features (10 premiÃ¨res):", feature_names[:10])
print("Classes:", target_names)

# Afficher 5 premiÃ¨res lignes sous forme de DataFrame pour mieux lire
pd.DataFrame(X, columns=feature_names).head()
```

**Q2.** Combien y a-t-il dâ€™exemples et de variables ?  
**Q3.** Que reprÃ©sente **une ligne** de `X` dans la rÃ©alitÃ© (pas â€œune ligne de tableauâ€ ğŸ˜‰) ?  
**Q4.** Que signifient les classes `target_names` (qui est 0 ? qui est 1) ?  
ğŸ‘‰ Indice : ce dataset est mÃ©dical (tumeur bÃ©nigne/maligne).

---

## Ã‰tape 2 â€” SÃ©parer apprentissage et test
```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.30, random_state=42, stratify=y
)
print(X_train.shape, X_test.shape)
```

**Q5.** Quelle proportion est rÃ©servÃ©e au test ? Pourquoi ne pas tout utiliser pour lâ€™entraÃ®nement **et** le test ?  
**Q6.** Ã€ quoi sert `stratify=y` ici ? (Astuce : garder la mÃªme proportion de classes.)

---

## Ã‰tape 3 â€” Construire un premier modÃ¨le k-NN (avec normalisation)
ComplÃ©tez le pipeline (lisez la doc `Pipeline` et `KNeighborsClassifier`) :

```python
# TODO: complÃ©tez les '...'
knn = Pipeline(steps=[
    ('scaler', StandardScaler()),          # Ã©tape de mise Ã  l'Ã©chelle
    ('clf', KNeighborsClassifier(n_neighbors=...))  # choisir k (entier > 0)
])

knn.fit(X_train, y_train)                  # apprentissage
y_pred_knn = knn.predict(X_test)           # prÃ©dictions
acc_knn = knn.score(X_test, y_test)        # accuracy
print(f"Accuracy k-NN (k=?): {acc_knn:.3f}")
```

**Q7.** Pourquoi **standardiser** les features avant k-NN ? (Il y a une histoire de **distance**â€¦)  
**Q8.** Expliquez avec vos mots ce que font `fit()` et `predict()`.

---

## Ã‰tape 4 â€” Lire une matrice de confusion (sur vos vrais rÃ©sultats)
```python
ConfusionMatrixDisplay.from_predictions(y_test, y_pred_knn)
plt.show()
```

**Q9.** Sans chercher sur Internet : dâ€™aprÃ¨s vos sorties, que reprÃ©sentent les 4 cases (nommez-les) ?  
**Q10.** Quelle information apporte la matrice de confusion que lâ€™accuracy ne montre pas ?

ğŸ‘‰ Besoin dâ€™un indice ? Voyez la doc `ConfusionMatrixDisplay` pour les libellÃ©s.

---

## Ã‰tape 5 â€” Choisir *k* de maniÃ¨re raisonnÃ©e (actif mais guidÃ©)
PlutÃ´t que de vous donner les valeurs, **proposez une liste de k impairs** allant de 1 Ã  une borne raisonnable.  
*RÃ¨gle pratique frÃ©quente* : tester jusquâ€™Ã  **â‰ˆ âˆš(n_train)** (arrondi au supÃ©rieur), en ne prenant que des **impairs** (pour Ã©viter les ex Ã¦quo en vote majoritaire).

```python
n_train = len(X_train)
k_max = int(np.ceil(np.sqrt(n_train)))    # borne haute recommandÃ©e
print("Taille train:", n_train, "â†’ k_max recommandÃ© â‰ˆ", k_max)

# TODO : construisez une liste de k impairs entre 1 et k_max (inclus si impair)
# Par ex. [1, 3, 5, ...]
k_values = [ ... ]

print(k_values[:10], "...")
```

**Q11.** Quelle mÃ©thode avez-vous utilisÃ©e pour gÃ©nÃ©rer des valeurs **impaires** ? (boucle, slicing, `range` pas de 2, etc.)

---

## Ã‰tape 6 â€” Ã‰valuer plusieurs k (boucle Ã  complÃ©ter)
ComplÃ©tez la boucle :

```python
results = []
for k in k_values:
    model = Pipeline(steps=[
        ('scaler', StandardScaler()),
        ('clf', KNeighborsClassifier(n_neighbors=...))   # TODO: utilisez k
    ])
    model.fit(X_train, y_train)
    acc = model.score(X_test, y_test)
    results.append((k, acc))

# Affichage des 10 premiers rÃ©sultats pour contrÃ´le
print(results[:10])
```

**Q12.** Quel est le **meilleur k** trouvÃ© ? Quelle **accuracy** obtenez-vous ?  
Astuce Python : `best = max(results, key=lambda t: t[1])`

Optionnel (trÃ¨s conseillÃ©) : tracer la courbe **k vs accuracy** avec `matplotlib` (une seule figure, pas de style spÃ©cifique demandÃ©).

---

## Ã‰tape 7 â€” Comparer avec la rÃ©gression logistique
ComplÃ©tez puis Ã©valuez :

```python
# TODO: pipeline StandardScaler + LogisticRegression
log_reg = Pipeline(steps=[
    ('scaler', StandardScaler()),
    ('clf', LogisticRegression(max_iter=500))   # vous pouvez ajuster max_iter si warning
])
log_reg.fit(X_train, y_train)
y_pred_log = log_reg.predict(X_test)
acc_log = log_reg.score(X_test, y_test)
print(f"Accuracy Logistique: {acc_log:.3f}")

ConfusionMatrixDisplay.from_predictions(y_test, y_pred_log)
plt.show()
```

**Q13.** Lequel est meilleur sur ce dataset : votre **meilleur k-NN** ou la **logistique** ?  
**Q14.** En regardant les deux matrices de confusion, quels **types dâ€™erreurs** diffÃ¨rent ? (FP vs FN par ex.)

ğŸ‘‰ Pour comprendre `LogisticRegression` : voyez la doc et le paramÃ¨tre `C` (rÃ©gularisation).

---

## Ã‰tape 8 â€” Effet de la normalisation (vÃ©rification indispensable)
**Test de contrÃ´le** : que se passe-t-il si vous **retirez le `StandardScaler`** pour k-NN ?  
(code quasi identique, mais sans lâ€™Ã©tape `scaler`)

```python
knn_no_scaler = Pipeline(steps=[
    # ('scaler', StandardScaler()),   # (dÃ©sactivÃ© volontairement)
    ('clf', KNeighborsClassifier(n_neighbors=...))  # reprenez votre meilleur k
])
knn_no_scaler.fit(X_train, y_train)
print("Accuracy k-NN sans scaler:", knn_no_scaler.score(X_test, y_test))
```

**Q15.** Que concluez-vous sur lâ€™importance de la **mise Ã  lâ€™Ã©chelle** pour k-NN ?  
ğŸ‘‰ Si la diffÃ©rence est faible chez vous : essayez dâ€™autres *k* et observez la matrice de confusion.

---

## Ã‰tape 9 â€” Mini-dÃ©fi ğŸ¯ (> 0.95 dâ€™accuracy)
- Visez **> 0.95** dâ€™accuracy : ajustez *k* ou des paramÃ¨tres de la logistique (`C`, `penalty`).  
- Justifiez en **1â€“2 phrases** votre choix final.

**Q16.** Quel est votre **meilleur modÃ¨le** ? ParamÃ¨tres et accuracy obtenue ?  
**Q17.** Quelle **limite** voyez-vous Ã  nâ€™utiliser que lâ€™accuracy ? (indice : classes dÃ©sÃ©quilibrÃ©es, coÃ»t des erreursâ€¦)

---

## Ã‰tape 10 â€” (Optionnel) Aller un peu plus loin
- Essayez `DecisionTreeClassifier` (doc : `sklearn.tree.DecisionTreeClassifier`).  
- Comparez rapidement lâ€™accuracy et la matrice de confusion.

**Q18.** Observez-vous un **sur-ajustement** (accuracy train >> test) ? Comment le dÃ©tecteriez-vous proprement Ã  lâ€™avenir ?
ğŸ‘‰ Teaser S2 : **validation croisÃ©e** et **biais/variance**.

---

## âœ… Ã€ rendre
- Notebook propre **avec votre code complÃ©tÃ©**, vos **rÃ©ponses Q1â€“Q18**, et vos figures (matrices de confusion, Ã©ventuelle courbe k vs accuracy).  
- Quelques phrases dâ€™interprÃ©tation pour **justifier** vos choix.
