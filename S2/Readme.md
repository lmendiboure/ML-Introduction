# TP ‚Äì S√©ance 2 
## Sur-apprentissage, sous-apprentissage et g√©n√©ralisation

### Objectifs
- Comprendre ce que signifient **underfitting** et **overfitting**.  
- Observer l‚Äô√©volution des performances sur **train** et **test**.  
- Visualiser la **fronti√®re de d√©cision** pour diff√©rents mod√®les.  
- Introduire la **validation crois√©e** comme m√©thode d‚Äô√©valuation plus robuste.  

üõ†Plateforme : **Google Colab** ou Python 3 + scikit-learn.  

---

## Rep√®res utiles (o√π chercher l'info)
- `make_moons` : https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html  
- `cross_val_score` : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html  
- `KNeighborsClassifier` : https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html  

Astuce Colab : utilisez `NomClasse?` pour voir rapidement les param√®tres disponibles.

---

## √âtape 0 ‚Äî Pr√©parer l‚Äôenvironnement

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_moons
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
```

**Q1.** Pourquoi garde-t-on `Pipeline` et `StandardScaler`, m√™me avec seulement deux variables ?  
**Q2.** V√©rifiez dans la doc de `make_moons` : que signifient les param√®tres `n_samples` et `noise` ?  

---

## √âtape 1 ‚Äî G√©n√©rer et visualiser les donn√©es

```python
X, y = make_moons(n_samples=300, noise=0.25, random_state=42)

plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.Set1, edgecolor="k")
plt.title("Dataset make_moons")
plt.show()
```

**Q3.** Quelle est la taille de ce dataset ?  
**Q4.** Que se passerait-il si on mettait `noise=0` ? Et si on augmentait fortement `noise` ?  
*(Indice : essayez et observez la figure)*  

---

## √âtape 2 ‚Äî S√©parer apprentissage et test

```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)
```

**Q5.** Quelle proportion est r√©serv√©e au test ?  
**Q6.** Pourquoi utilise-t-on encore `stratify=y` ici, alors que les classes semblent √©quilibr√©es ?  

---

## √âtape 3 ‚Äî Tester plusieurs valeurs de k

Compl√©tez le code (remplacez les `...`) :

```python
k_values = range(1, 21)   # valeurs de k √† tester
acc_train, acc_test = [], []

for k in k_values:
    model = Pipeline([
        ("scaler", StandardScaler()),
        ("knn", KNeighborsClassifier(n_neighbors=...))  # √† compl√©ter
    ])
    model.fit(..., ...)   # √† compl√©ter
    acc_train.append(model.score(X_train, y_train))
    acc_test.append(model.score(..., ...))  # √† compl√©ter

plt.plot(k_values, acc_train, label="train")
plt.plot(k_values, acc_test, label="test")
plt.xlabel("k")
plt.ylabel("accuracy")
plt.legend()
plt.show()
```

**Q7.** Pour quels k observe-t-on un **overfitting** (train ‚â´ test) ?  
**Q8.** Pour quels k observe-t-on un **underfitting** (train ‚âà test mais faible) ?  
**Q9.** Quel k semble √™tre un bon compromis ?  

---

## √âtape 4 ‚Äî Visualiser la fronti√®re de d√©cision

```python
from matplotlib.colors import ListedColormap

def plot_decision_boundary(model, X, y, title):
    h = .02
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, cmap=plt.cm.Pastel1, alpha=0.8)
    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor="k")
    plt.title(title)
    plt.show()

# Exemple avec k=1 et k=15
for k in [1, 15]:
    model = Pipeline([
        ("scaler", StandardScaler()),
        ("knn", KNeighborsClassifier(n_neighbors=k))
    ])
    model.fit(X_train, y_train)
    plot_decision_boundary(model, X, y, f"Fronti√®re pour k={k}")
```

**Q10.** D√©crivez la fronti√®re obtenue avec k=1. Est-elle simple ou complexe ?  
**Q11.** D√©crivez la fronti√®re obtenue avec k=15. Est-elle simple ou complexe ?  
**Q12.** Quelle relation observez-vous entre la complexit√© de la fronti√®re et le risque d‚Äôover/underfitting ?  

---

## √âtape 5 ‚Äî Validation crois√©e

```python
scores = cross_val_score(
    Pipeline([
        ("scaler", StandardScaler()),
        ("knn", KNeighborsClassifier(n_neighbors=5))
    ]),
    X, y, cv=5
)
print("Scores de validation crois√©e:", scores)
print("Moyenne:", scores.mean())
```

**Q13.** Qu‚Äôest-ce que `cv=5` signifie dans cet exemple ?  
**Q14.** Pourquoi la validation crois√©e est-elle plus fiable qu‚Äôun simple train/test split ?  
**Q15.** Dans quels cas pensez-vous qu‚Äôelle est particuli√®rement utile ?  

---

## √âtape 6 ‚Äî Discussion et synth√®se

**Q16.** R√©sumez avec vos mots la diff√©rence entre underfitting et overfitting.  
**Q17.** Si vous deviez conseiller un choix de k √† un coll√®gue, quelle d√©marche suivriez-vous (indices : courbes, CV) ?  
**Q18.** Quel est le lien entre ce TP et ce que vous avez fait avec le dataset Breast Cancer lors de la s√©ance 1 ?  
**Q19.** (Recherche) Regardez la doc de `KNeighborsClassifier`. Quels autres param√®tres que `n_neighbors` peuvent influencer le mod√®le ? Essayez-en un et observez l‚Äôeffet.  

---

# Bilan attendu
√Ä la fin de ce TP, vous devez √™tre capables de :  
- Expliquer et reconna√Ætre overfitting et underfitting.  
- Interpr√©ter une courbe train/test en fonction d‚Äôun hyperparam√®tre.  
- Utiliser la validation crois√©e pour √©valuer un mod√®le.  
- Comprendre que le choix des **hyperparam√®tres** est li√© au compromis biais/variance.
